{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Practical 3: Comparative Analysis of Different CNN Architectures\n",
    "## Deep Learning (AI302)\n",
    "\n",
    "**Objective:** Implement, train, and evaluate multiple CNN architectures to analyze the impact of network depth, loss functions, and optimization strategies on classification performance.\n",
    "\n",
    "### Assignment Structure:\n",
    "- **Part 1:** Implement multiple CNN architectures (LeNet-5, AlexNet, VGGNet, ResNet)\n",
    "- **Part 2:** Compare different loss functions and optimizers\n",
    "- **Part 3:** Visualize learned features using t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: CNN Architecture Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    \"\"\"LeNet-5 Architecture\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Test LeNet-5\n",
    "model = LeNet5()\n",
    "print(f\"LeNet-5 Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 AlexNet (Adapted for CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    \"\"\"AlexNet Architecture (adapted for CIFAR-10)\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 4 * 4, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Test AlexNet\n",
    "model = AlexNet()\n",
    "print(f\"AlexNet Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    \"\"\"VGG-like Network (adapted for CIFAR-10)\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Test VGGNet\n",
    "model = VGGNet()\n",
    "print(f\"VGGNet Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 ResNet (with Residual Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual Block for ResNet\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"ResNet Architecture\"\"\"\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, \n",
    "                         stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, return_features=False):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avg_pool(x)\n",
    "        features = x.view(x.size(0), -1)\n",
    "        x = self.fc(features)\n",
    "        if return_features:\n",
    "            return x, features\n",
    "        return x\n",
    "\n",
    "def ResNet50(num_classes=10):\n",
    "    \"\"\"ResNet-50\"\"\"\n",
    "    return ResNet(ResidualBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "# Test ResNet-50\n",
    "model = ResNet50()\n",
    "print(f\"ResNet-50 Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loss Functions and Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Focal Loss Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "print(\"✓ Focal Loss implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ArcFace Loss Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceLoss(nn.Module):\n",
    "    \"\"\"ArcFace Loss for feature learning\"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50):\n",
    "        super(ArcFaceLoss, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        \n",
    "    def forward(self, features, labels):\n",
    "        features = F.normalize(features, p=2, dim=1)\n",
    "        weight = F.normalize(self.weight, p=2, dim=1)\n",
    "        cosine = F.linear(features, weight)\n",
    "        theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "        one_hot = F.one_hot(labels, self.out_features).float()\n",
    "        theta_m = theta + self.m * one_hot\n",
    "        cosine_m = torch.cos(theta_m)\n",
    "        output = cosine_m * self.s\n",
    "        return F.cross_entropy(output, labels)\n",
    "\n",
    "print(\"✓ ArcFace Loss implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10_loaders(batch_size=128):\n",
    "    \"\"\"Load CIFAR-10 dataset\"\"\"\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                           download=True, transform=transform_train)\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                          download=True, transform=transform_test)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return trainloader, testloader\n",
    "\n",
    "# Load data\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "trainloader, testloader = get_cifar10_loaders(batch_size=128)\n",
    "print(f\"✓ Training batches: {len(trainloader)}\")\n",
    "print(f\"✓ Testing batches: {len(testloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, criterion, optimizer, epochs, device, model_name=\"Model\"):\n",
    "    \"\"\"Train a model\"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(trainloader, desc=f'{model_name} Epoch {epoch+1}/{epochs}')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}', \n",
    "                            'acc': f'{100.*correct/total:.2f}%'})\n",
    "        \n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        print(f'{model_name} Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "    \n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "def evaluate_model(model, testloader, device):\n",
    "    \"\"\"Evaluate a model\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiments as per assignment requirements\n",
    "experiments = [\n",
    "    {\n",
    "        'model': VGGNet(num_classes=10),\n",
    "        'model_name': 'VGGNet',\n",
    "        'optimizer_name': 'Adam',\n",
    "        'loss_name': 'BCE',\n",
    "        'epochs': 10\n",
    "    },\n",
    "    {\n",
    "        'model': AlexNet(num_classes=10),\n",
    "        'model_name': 'AlexNet',\n",
    "        'loss_name': 'Focal Loss',\n",
    "        'optimizer_name': 'SGD',\n",
    "        'epochs': 20\n",
    "    },\n",
    "    {\n",
    "        'model': ResNet50(num_classes=10),\n",
    "        'model_name': 'ResNet',\n",
    "        'loss_name': 'ArcFace',\n",
    "        'optimizer_name': 'Adam',\n",
    "        'epochs': 15\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {exp['model_name']} with {exp['optimizer_name']} and {exp['loss_name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = exp['model']\n",
    "    \n",
    "    # Setup loss function\n",
    "    if exp['loss_name'] == 'BCE':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif exp['loss_name'] == 'Focal Loss':\n",
    "        criterion = FocalLoss(alpha=1, gamma=2)\n",
    "    elif exp['loss_name'] == 'ArcFace':\n",
    "        criterion = ArcFaceLoss(in_features=512, out_features=10)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    if exp['optimizer_name'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    elif exp['optimizer_name'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    # Train model\n",
    "    train_losses, train_accuracies = train_model(\n",
    "        model, trainloader, criterion, optimizer, \n",
    "        exp['epochs'], device, exp['model_name']\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    test_accuracy = evaluate_model(model, testloader, device)\n",
    "    \n",
    "    result = {\n",
    "        'Model': exp['model_name'],\n",
    "        'Optimizer': exp['optimizer_name'],\n",
    "        'Epochs': exp['epochs'],\n",
    "        'Loss Function': exp['loss_name'],\n",
    "        'Training Accuracy': train_accuracies[-1],\n",
    "        'Testing Accuracy': test_accuracy\n",
    "    }\n",
    "    results.append(result)\n",
    "    trained_models[f\"{exp['model_name']}_{exp['loss_name']}\"] = model\n",
    "    \n",
    "    print(f\"\\n{exp['model_name']} Results:\")\n",
    "    print(f\"  Training Accuracy: {train_accuracies[-1]:.2f}%\")\n",
    "    print(f\"  Testing Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature Visualization with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, dataloader, device, num_samples=1000):\n",
    "    \"\"\"Extract features for visualization\"\"\"\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            if len(features_list) * inputs.size(0) >= num_samples:\n",
    "                break\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Get features before final layer\n",
    "            if hasattr(model, 'fc'):\n",
    "                if 'return_features' in model.forward.__code__.co_varnames:\n",
    "                    _, features = model(inputs, return_features=True)\n",
    "                else:\n",
    "                    x = F.relu(model.bn1(model.conv1(inputs)))\n",
    "                    x = model.layer1(x)\n",
    "                    x = model.layer2(x)\n",
    "                    x = model.layer3(x)\n",
    "                    x = model.layer4(x)\n",
    "                    x = model.avg_pool(x)\n",
    "                    features = x.view(x.size(0), -1)\n",
    "            elif hasattr(model, 'features'):\n",
    "                features = model.features(inputs)\n",
    "                features = features.view(features.size(0), -1)\n",
    "            else:\n",
    "                features = model(inputs)\n",
    "            \n",
    "            features_list.append(features.cpu().numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "    \n",
    "    features = np.concatenate(features_list, axis=0)[:num_samples]\n",
    "    labels = np.concatenate(labels_list, axis=0)[:num_samples]\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "def plot_tsne(features, labels, title):\n",
    "    \"\"\"Plot t-SNE visualization\"\"\"\n",
    "    print(f\"Computing t-SNE for {title}...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    features_2d = tsne.fit_transform(features)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], \n",
    "                         c=labels, cmap='tab10', alpha=0.6, s=20)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize features from different models\n",
    "for model_key, model in trained_models.items():\n",
    "    print(f\"\\nExtracting features from {model_key}...\")\n",
    "    features, labels = extract_features(model, testloader, device, num_samples=1000)\n",
    "    plot_tsne(features, labels, f\"t-SNE Visualization: {model_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "1. **Architecture Comparison:**\n",
    "   - Deeper networks (ResNet) typically achieve higher accuracy\n",
    "   - Skip connections help train deeper networks more effectively\n",
    "   \n",
    "2. **Loss Function Analysis:**\n",
    "   - Cross-Entropy: Standard baseline, works well for balanced datasets\n",
    "   - Focal Loss: Better handles difficult examples\n",
    "   - ArcFace: Produces more discriminative features with better class separation\n",
    "   \n",
    "3. **Optimization Strategies:**\n",
    "   - Adam: Faster convergence, adaptive learning rates\n",
    "   - SGD with momentum: More stable, may generalize better\n",
    "   \n",
    "4. **Feature Visualization:**\n",
    "   - t-SNE plots reveal how well models separate classes\n",
    "   - Different loss functions create different feature spaces\n",
    "   - Better separation indicates better learned representations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
